<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Vert.x virtual threads incubator</title><link rel="alternate" href="https://vertx.io/blog/vertx-virtual-threads-incubator" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/vertx-virtual-threads-incubator</id><updated>2022-07-27T00:00:00Z</updated><content type="html">JEP 425: Virtual threads aka Loom is coming to the Java Platform as a preview in Java 19. The Vert.x team is launching an incubator project to experiment with virtual threads.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2022/07/21/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2022/07/21/resteasy-releases/</id><updated>2022-07-21T18:11:11Z</updated><dc:creator /></entry><entry><title>How to use Go Toolset container images</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" /><author><name>Alejandro Sáez Morollón</name></author><id>011e5016-34ab-4570-8e1a-575cc4281eec</id><updated>2022-07-21T07:00:00Z</updated><published>2022-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_developer_tools/2019.1/html-single/using_go_toolset/index"&gt;Go Toolset package&lt;/a&gt; delivers the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; with Federal Information Processing Standard (FIPS) support for cryptographic modules and the &lt;a href="https://github.com/go-delve/delve"&gt;Delve debugger&lt;/a&gt; to &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; customers. We introduced this package &lt;a href="https://developers.redhat.com/blog/2017/10/31/getting-started-go-toolset"&gt;a few years ago&lt;/a&gt;. Now we also provide Go Toolset in container images. This article illustrates how these images support modern Go development.&lt;/p&gt; &lt;h2&gt;Obtaining Go Toolset container images&lt;/h2&gt; &lt;p&gt;The images are in Red Hat's &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;container image catalog&lt;/a&gt;. Search for &lt;code&gt;go-toolset&lt;/code&gt; &lt;a href="https://catalog.redhat.com/software/containers/search?q=go-toolset"&gt;here&lt;/a&gt;. As of this writing, we have images based on Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI) 7 and 8. To learn more about UBI, check the article &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat UBI is a Verified Publisher on Docker Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you don't want to use the Go Toolset container image, you can still install the Go Toolset package inside a ubi8 container. Just run &lt;code&gt;dnf install go-toolset&lt;/code&gt; inside the &lt;code&gt;registry.access.redhat.com/ubi8&lt;/code&gt; image, and you'll be all set.&lt;/p&gt; &lt;h2&gt;Pull the latest version of the Go Toolset image&lt;/h2&gt; &lt;p&gt;Using your container engine is the best way to pull down a container image. Let's use &lt;a href="https://podman.io"&gt;Podman&lt;/a&gt; for this example. The following commands pull down the latest version of the image based on UBI 8 and run a shell inside it:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ podman pull registry.access.redhat.com/ubi8/go-toolset:latest [alex@lab ~]$ podman run --rm -it go-toolset /bin/bash bash-4.4$ go version go version go1.17.7 linux/amd64 bash-4.4$ exit exit [alex@lab ~]$ &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using the image in a multistage environment&lt;/h2&gt; &lt;p&gt;You can refer to the image in a Dockerfile like any other image and use it, for example, as a build step in a multistage Dockerfile in tandem with the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-micro/5ff3f50a831939b08d1b832a"&gt;Red Hat Universal Base Image 8 Micro image&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="Dockerfile"&gt;FROM ubi8/go-toolset as build COPY ./src . RUN go mod init my_app &amp;&amp; \ go mod tidy &amp;&amp; \ go build . FROM ubi8/ubi-micro COPY --from=build /opt/app-root/src/my_app . CMD ./my_app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The ubi-micro image takes up less than 40MB, so the surface of your container is tiny but holds all of the great features UBI delivers.&lt;/p&gt; &lt;h2&gt;Go Toolset works with Toolbox&lt;/h2&gt; &lt;p&gt;I won’t be lying if I say &lt;a href="https://github.com/containers/toolbox"&gt;Toolbox&lt;/a&gt; is one of my favorite software tools. It allows you to keep working with your files and configurations inside a new container. I use Toolbox every day, and it works wonderfully with the Go Toolset image.&lt;/p&gt; &lt;p&gt;You can install Toolbox in both Red Hat Enterprise Linux and Fedora with &lt;code&gt;dnf install toolbox&lt;/code&gt;. The &lt;code&gt;toolbox&lt;/code&gt; command lets you install other resources:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ cat /etc/redhat-release Fedora release 35 (Thirty Five) [alex@lab ~]$ toolbox create --image registry.access.redhat.com/ubi8/go-toolset [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ cat /etc/redhat-release Red Hat Enterprise Linux release 8.6 (Ootpa) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you have all the files, configurations, and packages included with the image.&lt;/p&gt; &lt;h2&gt;Attach VS Code for IDE support&lt;/h2&gt; &lt;p&gt;You can improve productivity by attaching Visual Studio Code, &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;VS Code&lt;/a&gt; to a running container. Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Remote Containers extension&lt;/a&gt; and execute &lt;strong&gt;Attach to Running Container&lt;/strong&gt;. There is no need to configure Git or Kerberos tokens; simply jump into the container and start working.&lt;/p&gt; &lt;p&gt;I use this process to play with the new versions of the container images we built and bootstrap projects such as Go.&lt;/p&gt; &lt;h2&gt;Go Toolset includes FIPS security&lt;/h2&gt; &lt;p&gt;One exciting feature supported by the golang package in Go Toolset is &lt;a href="https://www.redhat.com/en/blog/how-rhel-8-designed-fips-140-2-requirements"&gt;FIPS 140-2 cryptographic modules&lt;/a&gt;. You can expect Go Toolset to follow the FIPS 140-2 security standard. Check for FIPS mode inside the Go Toolset container image:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ fips-mode-setup --check FIPS mode is enabled. [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ fips-mode-setup --check FIPS mode is enabled. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Enterprise-ready Go in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;The Go Toolset package is available as an image and integrates smoothly with other popular developer tools. By following the instructions I have provided, you can quickly become a more productive Go programmer in the cloud.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" title="How to use Go Toolset container images"&gt;How to use Go Toolset container images&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alejandro Sáez Morollón</dc:creator><dc:date>2022-07-21T07:00:00Z</dc:date></entry><entry><title>Get started with OpenShift Service Registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" /><author><name>Evan Shortiss</name></author><id>fa17af9f-46e7-4502-8b24-a99e6890c3ed</id><updated>2022-07-20T16:00:00Z</updated><published>2022-07-20T16:00:00Z</published><summary type="html">&lt;p&gt;Red Hat OpenShift Service Registry is a fully hosted and managed service that provides an API and schema registry for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. OpenShift Service Registry makes it easy for development teams to publish, discover, and reuse APIs and schemas.&lt;/p&gt; &lt;p&gt;Well-defined API and schema definitions are essential to delivering robust microservice and event streaming architectures. Development teams can use a registry to manage these artifacts in various formats, including &lt;a href="https://swagger.io/specification/"&gt;OpenAPI&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt;, &lt;a href="https://avro.apache.org/"&gt;Apache Avro&lt;/a&gt;, &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protocol Buffers&lt;/a&gt;, and more. Data producers and consumers can then use the artifacts to validate and serialize or deserialize data.&lt;/p&gt; &lt;p&gt;This article gets you started with OpenShift Service Registry. You’ll create a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application that uses the registry to manage schemas for data sent through topics in an &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; cluster. The tutorial should take less than 30 minutes, and involves the following steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a Red Hat Hybrid Cloud account.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Service Registry instance.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Streams for Apache Kafka instance.&lt;/li&gt; &lt;li&gt;Create Kafka topics.&lt;/li&gt; &lt;li&gt;Create a service account to facilitate authenticated access to your Kafka and Service Registry instances.&lt;/li&gt; &lt;li&gt;Build and run a Java application.&lt;/li&gt; &lt;/ol&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Schemas and API definitions are metadata that represent a contract between decoupled services, so they must be discoverable, documented, and assigned versions to track their evolution over time.&lt;/p&gt; &lt;h2&gt;About OpenShift Service Registry&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Service Registry is based on the open source &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry project&lt;/a&gt;. It provides a highly available service registry instance that is secure and compatible with both the &lt;a href="https://docs.confluent.io/platform/current/schema-registry/develop/api.html"&gt;Confluent Schema Registry API&lt;/a&gt; and &lt;a href="https://github.com/cloudevents/spec/blob/master/schemaregistry/schemaregistry.md"&gt;CNCF Schema Registry API&lt;/a&gt;. OpenShift Service Registry is also a perfect companion service for applications that use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You need a Red Hat Hybrid Cloud account to run the examples in this article. Create an account for free at &lt;a href="https://console.redhat.com/"&gt;console.redhat.com&lt;/a&gt;. You also need the following tools in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Java 11 or higher&lt;/li&gt; &lt;li&gt;Maven 3.8.1 or higher&lt;/li&gt; &lt;li&gt;Git&lt;/li&gt; &lt;li&gt;Your favorite IDE or text editor&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Creating a Service Registry instance&lt;/h2&gt; &lt;p&gt;Organizations or individuals with a Red Hat Hybrid Cloud account are entitled to a two-month trial instance of OpenShift Service Registry. To create an instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log in to your account on &lt;a href="https://console.redhat.com"&gt;console.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;In the user interface (UI), select &lt;strong&gt;Application Services&lt;/strong&gt; from the menu on the left.&lt;/li&gt; &lt;li&gt;Expand the &lt;strong&gt;Service Registry&lt;/strong&gt; entry on the side menu and click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link. Acknowledge the warning that it is a beta service.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Service Registry instance&lt;/strong&gt; button. A modal dialog will be displayed.&lt;/li&gt; &lt;li&gt;Enter a name for your Service Registry instance and click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your OpenShift Service Registry instance will be ready to use in a minute or two. A green checkmark will be displayed in the &lt;strong&gt;Status&lt;/strong&gt; column to indicate when the instance is ready, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service_registry_ready.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/service_registry_ready.png?itok=2I9j3fwW" width="1440" height="807" alt="When a green checkmark and a Ready status are displayed in the OpenShift Service Registry UI, the Service Registry instance can be opened." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Service Registry instance listed in the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Once your instance is ready, click on its row in the portal to view connection information, as shown in Figure 2. Take note of the &lt;strong&gt;Core Registry API&lt;/strong&gt; value because you'll need it soon.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;img alt="The service Registry instance connection information displayed in the UI." data-entity-type="file" data-entity-uuid="4e2e48ce-fc0e-417f-ab08-001c0c7068ee" src="https://developers.redhat.com/sites/default/files/inline-images/dblog-screen_0_0.png" width="1660" height="938" loading="lazy" /&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Service Registry instance connection information displayed in the UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Integrating Java applications with Service Registry&lt;/h2&gt; &lt;p&gt;Kafka producer applications can use serializers to encode messages that conform to a specific event schema. Kafka consumer applications can then use deserializers to validate that messages were serialized using the correct schema, based on a specific schema ID. This process is illustrated in Figure 3. You'll test serialization and deserialization using Java producer and consumer applications that connect to Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka_schema.png?itok=8Rp6rq9H" width="1440" height="841" alt="Both producers and consumers in Kafka get schemas from the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka producer and consumer applications using the OpenShift Service Registry to share schemas. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h3&gt;Provision a managed Kafka instance and create topics&lt;/h3&gt; &lt;p&gt;To get started, you'll need to create an OpenShift Streams for Apache Kafka instance and two topics: one named &lt;code&gt;quote-requests&lt;/code&gt; and the other named &lt;code&gt;quotes&lt;/code&gt;. We've explained how to obtain this runtime environment for free in &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka#provision_a_kafka_cluster_with_openshift_streams_for_apache_kafka_through_the_ui"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Remember to take note of your Kafka instance's bootstrap server URL. You will need this URL soon.&lt;/p&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;A service account is required to connect applications to the OpenShift Service Registry and OpenShift Streams for Apache Kafka instances. The service account provides a client ID and client secret that applications use to authenticate against the cloud services.&lt;/p&gt; &lt;p&gt;To create a service account:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Visit &lt;a href="https://console.redhat.com/beta/application-services/service-accounts"&gt;console.redhat.com/beta/application-services/service-accounts&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create service&lt;/strong&gt; account button.&lt;/li&gt; &lt;li&gt;Enter a name for the service account.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;The client ID and client secret will be displayed. Copy these down someplace safe.&lt;/li&gt; &lt;li&gt;Close the modal dialog.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Deploying the producer Java application&lt;/h2&gt; &lt;p&gt;At this point you have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift Service Registry instance&lt;/li&gt; &lt;li&gt;An OpenShift Streams for Apache Kafka instance&lt;/li&gt; &lt;li&gt;A service account for connecting applications to the previous two instances&lt;/li&gt; &lt;li&gt;Kafka topics to hold messages published by a producer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, it's time to deploy a producer application that publishes messages to your Kafka topic. This application utilizes an Avro schema to encode messages in Avro format. It also publishes this schema to your OpenShift Service Registry. Consumer applications can fetch the schema from OpenShift Service Registry to deserialize and validate records they consume from your Kafka topic.&lt;/p&gt; &lt;p&gt;The source code for both the producer and the consumer is available in this &lt;a href="https://github.com/evanshortiss/rhosr-quarkus-kafka-apicurio"&gt;GitHub repository&lt;/a&gt;. Clone it into your development environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone $REPOSITORY_URL rhosr-getting-started&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open the &lt;code&gt;rhosr-getting-started&lt;/code&gt; project using your preferred IDE or text editor, and open the &lt;code&gt;producer/pom.xml&lt;/code&gt; file. This file contains typical dependencies that are used to connect to Kafka and expose REST services. The &lt;code&gt;quarkus-apicurio-registry-avro&lt;/code&gt; dependency is used to generate Java classes based on Avro schema definitions. It also brings in dependencies required to work with the service registry, such as service registry-aware Kafka serializers and deserializers.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;producer/src/main/avro/quote.avsc&lt;/code&gt; file. This file contains an Avro schema defined using JSON. This schema can be used to generate a &lt;code&gt;Quote.java&lt;/code&gt; class that extends and implements the necessary Avro class and interface. The &lt;code&gt;Quote&lt;/code&gt; class is used to serialize outgoing messages to the underlying Kafka topic, and by the &lt;code&gt;quotes&lt;/code&gt; channel to deserialize incoming messages. The generated class can be found in the &lt;code&gt;target/generated-sources/Quota.java&lt;/code&gt; file after compiling the application or running it in development mode.&lt;/p&gt; &lt;p&gt;Lastly, examine the &lt;code&gt;producer/src/main/resource/application.properties&lt;/code&gt; file. This file configures the application to connect to a Kafka instance, register schemas with a registry, and use Avro serialization and deserialization.&lt;/p&gt; &lt;h3&gt;Run the producer application&lt;/h3&gt; &lt;p&gt;You can run the producer application wherever you like, including on an OpenShift cluster. I'll demonstrate how you can run the producer in your local development environment.&lt;/p&gt; &lt;p&gt;First, define the following environment variables in a shell. Replace the text within &lt;strong&gt;&lt;&gt;&lt;/strong&gt; angle brackets with the values you found in previous sections:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Used to authenticate against the registry and kafka cluster export CLIENT_ID=&lt;your-client-id&gt; export CLIENT_SECRET=&lt;your-client-secret&gt; # Used to connect to and authenticate against the service registry export OAUTH_SERVER_URL=https://sso.redhat.com/auth export REGISTRY_URL=&lt;core-service-registry-url&gt; # Used to connect to and authenticate against the kafka cluster export BOOTSTRAP_SERVER=&lt;kafka-bootstrap-url&gt; export OAUTH_TOKEN_ENDPOINT_URI=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once these values are defined, you can start the producer application in the same shell using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev -f ./producer/pom.xml -Dquarkus-profile=prod&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Quarkus application is now running in and has exposed an HTTP server on &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;. Use the following command to send a POST request that creates a quote and sends it to the &lt;code&gt;quote-requests&lt;/code&gt; Kafka topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST http://localhost:8080/quotes/request&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should receive a response in JSON format that contains a unique quote &lt;code&gt;id&lt;/code&gt; and random &lt;code&gt;price&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;View the Quote schema in Service Registry&lt;/h3&gt; &lt;p&gt;When you start the producer application and make a request to the &lt;code&gt;/quotes/request&lt;/code&gt; endpoint, the producer gets ready to send data to your Kafka topic. Prior to sending the data, the producer checks that the Quote Avro schema is available in OpenShift Service Registry. If the Quote schema is not found, the producer publishes the schema to the registry. The producer then serializes the outgoing data using the schema and includes the registered schema ID in the message value.&lt;/p&gt; &lt;p&gt;A downstream consumer application can use the schema ID found in the message payload to fetch the necessary schema from the registry. The consumer application can then use the schema to validate and deserialize the incoming message.&lt;/p&gt; &lt;p&gt;To confirm that the Avro schema was published to OpenShift Service Registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to your &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;OpenShift Service Registry Instances&lt;/a&gt; listing.&lt;/li&gt; &lt;li&gt;Select the instance used by your producer application.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Artifacts&lt;/strong&gt; tab.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You should see the Quote schema, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/quote_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/quote_schema.png?itok=6Gj9pQGo" width="1440" height="810" alt="The Quote Avro schema is listed in the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Quote Avro schema listed in OpenShift Service Registry. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Select the Quote schema in the list and view the &lt;strong&gt;Content&lt;/strong&gt; tab. Use the &lt;strong&gt;Format&lt;/strong&gt; button to improve the legibility of the JSON, and confirm that it matches the &lt;code&gt;Quote.avsc&lt;/code&gt; file in the producer application codebase.&lt;/p&gt; &lt;h2&gt;Consuming messages in Service Registry&lt;/h2&gt; &lt;p&gt;The repository you cloned as part of this exercise contains a consumer application. This consumer application is configured using the same environment variables as the producer and reads messages from the &lt;code&gt;quote-requests&lt;/code&gt; topic. Because the producer and consumer use OpenShift Service Registry, the consumer can fetch the necessary Avro schema to validate and deserialize incoming quote requests.&lt;/p&gt; &lt;p&gt;Run the producer and consumer at the same time. Use cURL to open a connection to the server-sent events (SSE) endpoint at &lt;a href="http://localhost:8080/quotes"&gt;http://localhost:8080/quotes&lt;/a&gt;, then use another HTTP client to POST to &lt;a href="http://localhost:8080/quotes/request"&gt;http://localhost:8080/quotes/request&lt;/a&gt;. The consumer should correctly deserialize and process the items from the &lt;code&gt;quote-requests&lt;/code&gt; topic and place the processed quote into the &lt;code&gt;quotes&lt;/code&gt; topic, after which the SSE endpoint should display the items as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/curl.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/curl.jpeg?itok=FEaLwNB0" width="1440" height="796" alt="A cURL command displays deserialized, JSON-formatted data received by the consumer at the SSE endpoint." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Deserialized, JSON-formatted data received by the consumer at the SSE endpoint and displayed by a cURL command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Enforcing schema compatibility rules&lt;/h2&gt; &lt;p&gt;OpenShift Service Registry supports various schema compatibility rules to prevent the publication of schema changes that could lead to incompatibilities with downstream applications (that is, breaking changes). You can read more about compatibility rules in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817"&gt;service documentation&lt;/a&gt;. To enable this enforcement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open the Service Registry UI at &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;console.redhat.com/beta/application-services/service-registry&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Select your instance and view the Quote schema.&lt;/li&gt; &lt;li&gt;Set the &lt;strong&gt;Validity Rule&lt;/strong&gt; to &lt;strong&gt;Full&lt;/strong&gt; and the &lt;strong&gt;Compatibility Rule&lt;/strong&gt; to &lt;strong&gt;Backward&lt;/strong&gt; (see Figure 6).&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Upload new version&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Paste in the following Avro schema and click &lt;strong&gt;Upload&lt;/strong&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt; &lt;code&gt;{ "namespace": "org.acme.kafka.quarkus", "type": "record", "name": "Quote", "fields": [ { "name": "id", "type": "string" }, { "name": "price", "type": "int" }, { "name": "notes", "type": "string" } ] }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Figure 6 shows these updates in the console.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rule.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rule.png?itok=j6E6m_0t" width="1440" height="817" alt="The Compatibility Rule can be set in the OpenShift Service Registry UI." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Enforcing schema compatibility rules using the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;An &lt;strong&gt;Invalid Content&lt;/strong&gt; error should be displayed, because this new schema violated the backward compatibility rule by adding a new required field. New fields must be optional if backward compatibility is enabled. As the error message indicated, new schemas are required to provide backward-compatible schemas for all future evolution.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations—in this article you have learned how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use OpenShift Service Registry.&lt;/li&gt; &lt;li&gt;Use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Create Avro schemas.&lt;/li&gt; &lt;li&gt;Integrate Java applications that use Avro schemas with both services.&lt;/li&gt; &lt;li&gt;Manage schema evolution and apply rules to prevent breaking changes for downstream consumers.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sign up for the services described in this article, and let us know your experience in the comments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" title="Get started with OpenShift Service Registry"&gt;Get started with OpenShift Service Registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2022-07-20T16:00:00Z</dc:date></entry><entry><title type="html">An improved &amp;#8216;Spreadsheet like&amp;#8217; experience on DMN Editor</title><link rel="alternate" href="https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html" /><author><name>Fabrizio Antonangeli</name></author><id>https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html</id><updated>2022-07-20T14:33:15Z</updated><content type="html">The boxed expression editor is a key component of the DMN Editor. In previous articles, we introduced the new implementation of this . In this article, I’ll show how I extended the component, implementing the keyboard navigation for faster DMN editing. REQUIREMENTS * (1.46.0+); * (0.20.0+); , there is a ready-to-use online version of the DMN editor to try the new functionality. THE NEW KEYBOARD NAVIGATION The editing of a decision in the DMN Editor was based on the mouse interaction requiring the user to continuously switch between keyboard and mouse, resulting in a time-consuming activity. As my first task on the project, I worked on implementing a user experience as much as possible similar to Google Spreadsheet. As a result, the user can edit an expression, cell by cell, seamlessly using only the keyboard. THE NEW KEYBOARD ACTIONS AVAILABLE IN “VIEW MODE” Navigation between cells is now available in any type of expression using the arrow keys to go UP, RIGHT, DOWN, and LEFT, in a natural way. Continuous navigation is available with TAB, to jump to the next cell, or SHIFT-TAB to jump to the previous cell. This with the exception that if you are at the end of the row, you jump to the first cell of the next row or the last cell of the previous row if you are on the first cell of the row. After you choose the cell you want to edit, you just start writing the content, and this way, you erase the already existing content, if there is any. In addition, if you just want to start editing from the end of the cell’s content, you just need to press ENTER and start typing your content. Differently from a normal Spreadsheet, we have particular cells with nested tables or cells that don’t have just text, and when you click on them, a pop-up with a few inputs will appear, used in different cases. In this last case, you can now open the pop-up by pressing ENTER, and then you jump between the inputs inside the form using TAB/SHIFT-TAB. ENTER/ESC will close the pop-up and save or cancel your changes. For the case of nested tables, for instance, a "Context expression" with a Decision Table inside, you can jump inside the nested table with ENTER key and come back to the parent table with ESC. THE NEW KEYBOARD ACTIONS AVAILABLE IN “EDIT MODE” When you finish editing a cell, and you want to apply your changes, using TAB/SHIFT-TAB you save and jump directly to the next or previous cell, or to the cell below with ENTER. On the contrary, you can press ESC to cancel your changes to the cell. A big change to the UX was the introduction of the "newline" in the Decision Table’s cells, using the CTRL-ENTER. This change impacted the logic of the "copy &amp;amp; paste" that was based on single-line cells. To achieve that, the new logic is based on the parser, and now you can just copy your data to or from a Spreadsheet. The implementation The work has been mainly focused on the package inside repository. The main obstacle to implementing all these functionalities was the communication between React custom and third-party components and highlighting the selected cell. The easiest way was possibly the use of the “contenteditable” attribute, but that required a full rewriting of the components for the table and cells. After an evaluation of 4 other solutions, we decided to listen to the keyboard events from the TD HTML element representing the cell, then show the highlight through the “:focus” CSS selector. This way managing the "onBlur" event or memorizing the selected cell is not needed. Instead, when the focus is on the input, which is actually a Monaco editor, the highlight needs to be on the component inside the cell, that has the state data. This is done by adding the CSS class "editable-cell–edit-mode" to the main tag of the component EditableCell. Then to ensure the stability of the component we use "Jest" and "@testing-library/react" to render components. For the E2E test, we use "Cypress", which currently doesn’t support TAB key simulation which we managed with the plugin. CONCLUSION Creating or modifying a Decision can be a time-consuming activity if you have a lot of data to add inside. Giving the user the ability to write that data quickly and in the way, he was used to, was very important to us. We also didn’t want the user to learn a new way for that, and we wanted to give the same experience as Google Spreadsheet or Excel. The post appeared first on .</content><dc:creator>Fabrizio Antonangeli</dc:creator></entry><entry><title>Git workflows: Best practices for GitOps deployments</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments" /><author><name>Christian Hernandez</name></author><id>9298f676-ac50-4d48-b863-304eb4a1e95b</id><updated>2022-07-20T07:00:00Z</updated><published>2022-07-20T07:00:00Z</published><summary type="html">&lt;p&gt;Your Git workflows are at the center of your &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; deployments because workflows are the means of implementing your changes in your environment. When you adopt GitOps, Git is not only your source of truth (as it is for most projects) but also your interface into your environment. Developers have used Git workflows for their application delivery method for years, and now operations teams will have to adopt similar workflows.&lt;/p&gt; &lt;p&gt;But there are key differences between how you manage your code in Git and how you manage your GitOps configuration in Git. Here I will go over these differences and describe the best practices you should follow to make the best use of Git workflows for your GitOps deployments. We will see how to separate your configuration from your code, how to use branches, how to use trunk-based development workflows effectively, and tips for setting up policies and security for your Git workflows.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;This is an excerpt from the &lt;em&gt;The Path to GitOps&lt;/em&gt;. &lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;Download the e-book today&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Separate your repositories&lt;/h2&gt; &lt;p&gt;There are a few things to keep in mind when setting up your Git workflows for your GitOps directory structure and GitOps in general. The first is to keep your application code in a separate repository from your YAML configurations. This might seem counterintuitive initially, but most teams that start with code and configurations together quickly learn it’s better to separate them. &lt;/p&gt; &lt;p&gt;There are a few reasons for separate repositories. First, you don’t want a configuration change (such as changing the scale of a deployment from three to four nodes) to trigger a rebuild of your application if your application code didn’t change. Another reason is that the approval process of getting a change into an environment shouldn’t hold back continuous integration of your code. In general, application code and configuration information have independent lifecycles.&lt;/p&gt; &lt;p&gt;Also, many organizations separate the deployment process into several different teams. A lot of the time, the operations or release management team takes care of the application’s release. Although DevOps aims to reduce barriers between teams and their activities, you don’t want one team’s process to slow down another.&lt;/p&gt; &lt;h2&gt;Separate development in directories, not branches&lt;/h2&gt; &lt;p&gt;Another best practice that surprises many programmers is to separate environments–such as test and production–into different directories, but not create branches for them. Like the separation of code and configurations, this principle might seem to go against the grain of version control, but keeping track of environmental branches can be a challenge.&lt;/p&gt; &lt;p&gt;One of the difficulties you might encounter if you manage workflows through branches is that promotion from one environment to another isn’t as simple as a merge. You can see this issue with a simple example of updating an image tag. The application has been built, tested, and deemed ready to go from a sandbox environment to a test environment. But updating the image tag comes with other changes you don’t want to merge. What about the scale in the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt;? What about the ConfigMaps and Secrets? Those are bound to change in different environments and include things that should not be merged into other environments.&lt;/p&gt; &lt;p&gt;In short, every environment has configuration details specific to that environment. You can manually make the changes one by one or &lt;a href="https://git-scm.com/docs/git-cherry-pick"&gt;"cherry-pick"&lt;/a&gt;, but then your "simple merge" is no longer that simple. When you’re constantly cherry-picking or making manual changes, the effort level outweighs the benefits of trying to mirror the application workflows.&lt;/p&gt; &lt;p&gt;Another danger of using branches and cherry-picking your way into production is that this will likely introduce a significant drift. As you get further along in the life of a software project, when it spawns hundreds of environments with dozens upon dozens of applications, you can quickly see how cherry-picking and making manual changes can get out of hand. You can no longer use &lt;a href="https://git-scm.com/docs/git-diff"&gt;a simple diff&lt;/a&gt; to see the differences between branches, as the differences will be astronomical. &lt;/p&gt; &lt;p&gt;In the world of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, the &lt;a href="https://kustomize.io"&gt;Kustomize&lt;/a&gt; patching framework, and the &lt;a href="https://helm.sh"&gt;Helm&lt;/a&gt; package manager, using branches for environments is an antipattern. Kustomize and Helm make using directories and overlays for your environments easier. Kustomize, in particular, allows you to have a core set of manifests (called a "base" in Kustomize) and store the deltas in directories (called "overlays" in Kustomize). You use these overlays as directories with specific environment configurations in these directories.&lt;/p&gt; &lt;p&gt;So do you use branches at all? Yes, but not in the way you think. With GitOps, trunk-based development has emerged as the development model for your configuration repositories.&lt;/p&gt; &lt;h2&gt;Trunk-based development&lt;/h2&gt; &lt;p&gt;The recommended workflow for implementing GitOps with Kubernetes manifests is known as &lt;a href="https://trunkbaseddevelopment.com/"&gt;trunk-based development&lt;/a&gt;. This method defines one branch as the "trunk" and carries out development on each environment in a different short-lived branch. When development is complete for that environment, the developer creates a pull request for the branch to the trunk. Developers can also create a fork to work on an environment, and then create a branch to merge the fork into the trunk.&lt;/p&gt; &lt;p&gt;Once the proper approvals are done, the pull request (or the branch from the fork) gets merged into the trunk. The branch for that feature is deleted, keeping your branches to a minimum. Trunk-based development trades branches for directories.&lt;/p&gt; &lt;p&gt;You can think of the trunk as a "main" or primary branch. production and prod are popular names for the trunk branch.&lt;/p&gt; &lt;p&gt;Trunk-based development came about to enable continuous integration and continuous delivery by supplying a development model focused on the fast delivery of changes to applications. But this model also works for GitOps repositories because it keeps things simple and more in tune with how Kustomize and Helm work. When you record deltas between environments, you can clearly see what changes will be merged into the trunk. You won’t have to cherry-pick nearly as often, and you’ll have the confidence that what is in your Git repository is what is actually going into your environment. This is what you want in a GitOps workflow.&lt;/p&gt; &lt;h2&gt;Pay attention to policies and security&lt;/h2&gt; &lt;p&gt;Part of the challenge with trunk-based development is that now there is a single branch where things can go wrong. When relying on Git as your source of truth, it can be quite scary to depend on a single branch for not only your production environment but your organization as a whole. So you need to pay special attention to the features that version control offers for policy management and security to protect your trunk and provide stability to your environment.&lt;/p&gt; &lt;p&gt;When setting up your Git repository policies, use &lt;a href="https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/defining-the-mergeability-of-pull-requests/about-protected-branches#about-branch-protection-rules"&gt;GitHub’s branch protection rules&lt;/a&gt; (or the equivalent from other Git providers). Setting branch protection rules provides several benefits, the most important of which is preventing someone from force pushing a change into the trunk (which in turn makes an immediate alteration to your environment). Branch protection also protects the branch from being accidentally or intentionally deleted. There are other advantages to protected branches, but the main takeaway is this: You need to trust what is in Git because it is in charge of managing your environment. Take every precaution that builds trust.&lt;/p&gt; &lt;p&gt;Also, set up rules as to who can perform a merge and when. Make sure that all affected parties in your organization see a proposed merge. For example, perhaps a network change should be approved not only by the system administration team but also by the networking team and the security team. A rule can take the form of a "minimum number of approvals," but that doesn’t limit the number of approvals to the minimum. And while you’ll have multiple approvers, you should allow only a handful of people to actually merge the change.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Read &lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;The Path to GitOps&lt;/a&gt; &lt;/em&gt;to discover where GitOps fits in your CI/CD (continuous integration/continuous delivery) pipelines and explore the various ways you can implement it. You'll learn about popular tools like Argo CD and Flux and see how Kustomize, Helm, and Kubernetes Operators make it easier to deal with lengthy configuration files.&lt;/p&gt; &lt;p&gt;Find even more GitOps resources from Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;Getting GitOps: A practical platform with OpenShift, Argo CD, and Tekton&lt;/a&gt;&lt;/em&gt; helps you put it all together by walking through a common use case from beginning to end.&lt;/li&gt; &lt;li&gt;Get a preview of &lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/gitops-cookbook"&gt;GitOps Cookbook&lt;/a&gt;&lt;/em&gt;, a collection of useful recipes to follow GitOps practices on Kubernetes.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments" title="Git workflows: Best practices for GitOps deployments"&gt;Git workflows: Best practices for GitOps deployments&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christian Hernandez</dc:creator><dc:date>2022-07-20T07:00:00Z</dc:date></entry><entry><title>Secure Kubernetes certificates with cert-manager and Dekorate</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate" /><author><name>Jose Carvajal Hilario, Ana-Maria Mihalceanu, Charles Moulliard</name></author><id>6a78a218-d650-4212-b182-8982603d1964</id><updated>2022-07-19T07:00:00Z</updated><published>2022-07-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cert-manager.io/"&gt;Cert-manager&lt;/a&gt; is a cloud-native certificate management service for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. To configure cert-manager, you need to install several resources using custom resource definitions (CRDs). Depending on the issuer type and the certificate you need, creating these custom resources can become complex. This article introduces &lt;a href="https://dekorate.io/"&gt;Dekorate&lt;/a&gt; as an easier way to generate the cert-manager custom resources. We will also provide an example &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application based on &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; that uses the certificate generated by cert-manager.&lt;/p&gt; &lt;h2&gt;Getting started with the Dekorate cert-manager extension&lt;/h2&gt; &lt;p&gt;Cert-manager requires the installation of several resources, including &lt;code&gt;Issuer&lt;/code&gt;, &lt;code&gt;ClusterIssuer&lt;/code&gt;, and &lt;code&gt;Certificate&lt;/code&gt;. Cert-manager processes these resources to populate a secret, containing authentication information, such as a CA certificate, private key, server certificate, or Java keystores. This secret can then be used to secure your application endpoints or &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Kubernetes Ingress&lt;/a&gt; resources.&lt;/p&gt; &lt;p&gt;Dekorate, starting with version 2.10, can generate the certificate and issuer resources for you. Include the cert-manager Dekorate dependency in your POM file using the latest version of Dekorate from &lt;a href="https://search.maven.org/search?q=a:certmanager-annotations%20AND%20g:io.dekorate"&gt;Maven central&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.dekorate&lt;/groupId&gt; &lt;artifactId&gt;certmanager-annotations&lt;/artifactId&gt; &lt;version&gt;{dekorate.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The minimal information Dekorate needs for certificate configuration is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;secretName&lt;/code&gt; property containing the name of the &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Kubernetes secret&lt;/a&gt; resource that will include the files generated by cert-manager.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;Issuer&lt;/code&gt; that represents the certificate authority (CA). The &lt;a href="https://dekorate.io/docs/cert-manager#issuers"&gt;Issuer section of the Dekorate documentation &lt;/a&gt; lists supported options.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can start with a minimal configuration in the &lt;code&gt;.properties&lt;/code&gt; file and set up the following keys:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.secret-name=tls-secret # The self-signed issuer: dekorate.certificate.self-signed.enabled=true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://dekorate.io/configuration-guide/#cert-manager"&gt;Dekorate Cert-Manager Configuration Guide&lt;/a&gt; lists many configuration options that determine how Dekorate works with cert-manager. You can specify configuration options by adding the &lt;code&gt;@Certificate&lt;/code&gt; annotation to your Java program:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;@Certificate(secretName = "tls-secret", selfSigned = @SelfSigned(enabled = true)) public class Main { // ... } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This configuration generates all the resources in the &lt;code&gt;target/classes/dekorate/kubernetes.yml&lt;/code&gt; file, which should look like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: kubernetes-example spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: kubernetes-example spec: encodeUsagesInRequest: false isCA: false issuerRef: name: kubernetes-example secretName: tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dekorate cert-manager extension considers the secret that contains the files generated by cert-manager and mounts it as a volume and as part of the deployment. Dekorate allows the application to access the files and configure the HTTPS/TLS endpoint:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: apps/v1 kind: Deployment metadata: name: kubernetes-example spec: replicas: 1 template: spec: containers: - name: kubernetes-example volumeMounts: - mountPath: /etc/certs name: volume-certs readOnly: true volumes: - name: volume-certs secret: optional: false secretName: tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Securing resources&lt;/h3&gt; &lt;p&gt;When securing your resources, it's important to validate that the requests are coming from known hosts. To add these trusted hosts, use the &lt;code&gt;dnsNames&lt;/code&gt; property for the certificate configuration. The following line, for example, adds a single hostname to the property:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.dnsNames=foo.bar.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The certificate will then allow only requests for the &lt;code&gt;foo.bar.com&lt;/code&gt; server host. Add multiple hosts by separating them with commas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If the DNS hostname does not exist, you will get an error.&lt;/p&gt; &lt;p&gt;In Kubernetes, you can publicly expose an application using Ingress resources, such as:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-example spec: rules: - host: foo.bar.com http: paths: - pathType: Prefix path: "/" backend: service: name: kubernetes-example port: number: 8080 tls: - hosts: - foo.bar.com secretName: tls-secret # &lt; cert-manager will store the created certificate in this secret. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dekorate can help you generate the previous Ingress resource definition by adding the following key properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.kubernetes.ingress.host=foo.bar.com dekorate.kubernetes.ingress.expose=true dekorate.kubernetes.ingress.tlsSecretName=tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring HTTPS/TLS with Dekorate cert-manager extension for a Spring Boot application&lt;/h2&gt; &lt;p&gt;The example in this section demonstrates how to configure an HTTPS/TLS microservice using the Dekorate cert-manager extension.&lt;/p&gt; &lt;h3&gt;Enabling HTTPS transport&lt;/h3&gt; &lt;p&gt;To enable the HTTPS/TLS transport in Spring Boot, add the following properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;server.port=8443 server.ssl.enabled=true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, configure the Java PKCS#12 keystore properties that your Spring Boot application will use to get the server certificate signed and obtain the private key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;server.ssl.key-store-type=PKCS12 server.ssl.key-store=/path/to/keystore.p12 server.ssl.key-store-password=the password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Since the keystore file is password protected, you need to create the secret that contains that password. This is where cert-manager comes into play. The following sections illustrate how to instruct cert-manager to generate the keystore and how to configure the application to mount and use the secret.&lt;/p&gt; &lt;h3&gt;Generating a self-signed certificate and keystore&lt;/h3&gt; &lt;p&gt;You can configure the Dekorate cert-manager extension to request the generation of the &lt;code&gt;keystore.p12&lt;/code&gt; PKCS#12 file and a self-signed certificate by specifying:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.secret-name=tls-secret dekorate.certificate.self-signed.enabled=true dekorate.certificate.keystores.pkcs12.create=true # the secret name of the password: dekorate.certificate.keystores.pkcs12.passwordSecretRef.name=pkcs12-pass dekorate.certificate.keystores.pkcs12.passwordSecretRef.key=password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Based on this configuration, Dekorate will create the &lt;code&gt;Certificate&lt;/code&gt; and &lt;code&gt;Issuer&lt;/code&gt; resources that you can install in Kubernetes. This resource will be used by the Certificate Manager to generate a self-signed certificate and the keystore files within a secret named &lt;code&gt;tls-secret&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To protect the keystore, create a secret named &lt;code&gt;pkcs12-pass&lt;/code&gt; in the &lt;code&gt;src/main/resources/k8s/common.yml&lt;/code&gt; file. The data field must include the key password which is encoded in base64. The following example, shows the "supersecret" string encoded in base64:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: v1 kind: Secret metadata: name: pkcs12-pass data: # "supersecret" in base64: password: c3VwZXJzZWNyZXQ= type: Opaque &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can instruct Dekorate to find the &lt;code&gt;common.yml&lt;/code&gt; under &lt;code&gt;src/main/resources/&lt;/code&gt; by setting the folder name (eg. &lt;code&gt;k8s&lt;/code&gt;) in the &lt;code&gt;dekorate.options.input-path&lt;/code&gt; property:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.options.input-path=k8s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After configuring these details and installing the resources created by Dekorate on the Kubernetes platform, cert-manager will generate the generated PKCS#12 keystore file named &lt;code&gt;keystore.p12&lt;/code&gt; within the &lt;code&gt;tls-secret&lt;/code&gt; secret.&lt;/p&gt; &lt;p&gt;The Dekorate cert-manager extension will also configure the Spring Boot application to automatically mount a volume using the &lt;code&gt;tls-secret&lt;/code&gt; secret at the path &lt;code&gt;/etc/certs&lt;/code&gt; (the path can be specified using the &lt;code&gt;dekorate.certificate.volume-mount-path&lt;/code&gt; property). Therefore, you can map the Keystore file and password into the &lt;code&gt;server.ssl.key-store&lt;/code&gt; and &lt;code&gt;server.ssl.key-store-password&lt;/code&gt; Spring Boot properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.kubernetes.env-vars[0].name=SERVER_SSL_KEY_STORE dekorate.kubernetes.env-vars[0].value=/etc/certs/keystore.p12 dekorate.kubernetes.env-vars[1].name=SERVER_SSL_KEY_STORE_PASSWORD dekorate.kubernetes.env-vars[1].secret=pkcs12-pass dekorate.kubernetes.env-vars[1].value=password &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Running the application in Kubernetes&lt;/h3&gt; &lt;p&gt;To run this example, you must have access to a Kubernetes cluster and &lt;a href="https://cert-manager.io/docs/installation/"&gt;install cert-manager&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Next, generate the manifests and push the application container image to your container registry. This example uses &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; as the container registry and &lt;code&gt;user&lt;/code&gt; as the group name, but you should substitute the values from your environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mvn clean install -Ddekorate.push=true -Ddekorate.docker.registry=quay.io -Ddekorate.docker.group=user &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you execute the previous command, install the generated manifests, which are available at &lt;code&gt;target/classes/META-INF/dekorate/kubernetes.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl apply -f target/classes/META-INF/dekorate/kubernetes.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a few moments, check for the secret resource named &lt;code&gt;tls-secret&lt;/code&gt; created by the &lt;code&gt;Cert-Manager&lt;/code&gt; in the form of a PKCS#12 keystore file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl get secret/tls-secret -o yaml | grep keystore.p12 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the status of your pods using the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl get pods -w &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If your application is running, the output looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NAME READY STATUS RESTARTS AGE spring-boot-with-certmanager-example-566546987c-nj94n 1/1 Running 0 2m23s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Try out the application by port-forwarding port 8443:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl port-forward spring-boot-with-certmanager-example-566546987c-nj94n 8443:8443 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now if you browse to &lt;code&gt;https://localhost:8443/&lt;/code&gt;, you should see &lt;code&gt;Hello world from HTTPS!&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Dekorate reduces the complexity of certification management&lt;/h2&gt; &lt;p&gt;In this article, you learned how to easily generate cert-manager custom resources using Dekorate, and how to use the generated secret by cert-manager for a Spring Boot application.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate" title="Secure Kubernetes certificates with cert-manager and Dekorate"&gt;Secure Kubernetes certificates with cert-manager and Dekorate&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jose Carvajal Hilario, Ana-Maria Mihalceanu, Charles Moulliard</dc:creator><dc:date>2022-07-19T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.10.3.Final released - Fixes CVE-2022-2466</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-10-3-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-10-3-final-released/</id><updated>2022-07-19T00:00:00Z</updated><content type="html">2.10.0.CR1 introduced a major security issue known as CVE-2022-2466 in the SmallRye GraphQL server extension and all the 2.10.x releases are affected (together with 2.11.0.CR1). 2.10.3.Final fixes it and the fix will also be included in the upcoming 2.11.0.Final. You are only affected by this issue if you are exposing...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Infinispan 14 OpenTelemetry tracing integration</title><link rel="alternate" href="https://infinispan.org/blog/2022/07/18/infinispan-14-opentelemetry-tracing" /><author><name>Fabio Massimo Ercoli</name></author><id>https://infinispan.org/blog/2022/07/18/infinispan-14-opentelemetry-tracing</id><updated>2022-07-18T19:37:00Z</updated><content type="html">Dear Infinispan community, With the Infinispan 14 development release 04, we started to support tracing with OpenTelemetry. If configured, Infinispan Server produces cache events tracing spans and sends them to a remote tracing collector server, such as Jaeger or Zipkin. Moreover, if a Java client application with the HotRot or the Rest client produces some tracing spans, these spans can be correlated as parent spans of the corresponding spans events produced by the Infinispan Server. SET UP TRACING ON INFINISPAN SERVER The new version of Infinispan Server comes with a gRPC OpenTelemetry Protocol (OTLP) Exporter, which is now supported by the majority of tracing servers. For instance, with the newer Jaeger server versions, you can enable data collection through the OTLP protocol with the following option: ./jaeger-all-in-one --collector.otlp.enabled The server opens a port to import gRPC OTLP tracing data at port 4317. Configure tracing on the Infinispan Server by setting system properties or environment variables: infinispan.tracing.enabled=true otel.service.name=infinispan-server-service otel.exporter.otlp.endpoint=http://localhost:4317 otel.metrics.exporter=none The first property is Infinispan specific and enables the tracing capability of the Infinispan Server. The further properties belong to the that Infinispan uses to configure the tracing exporter. In this case, OTLP gRPC Exporter protocol is used and the server runs on the same machine as the Infinispan Server. Starting the server with these parameters: export JAVA_OPTS="-Dinfinispan.tracing.enabled=true -Dotel.service.name=infinispan-server-service -Dotel.exporter.otlp.endpoint=http://localhost:4317 -Dotel.metrics.exporter=none" ./server.sh The following log is produced when the server starts: (ForkJoinPool.commonPool-worker-2) [org.infinispan.server.core.telemetry.TelemetryServiceFactory] ISPN000952: OpenTelemetry instance loaded: OpenTelemetrySdk{... This indicates that the OpenTelemetrySdk is correctly configured. TRACING FROM A HOTROD CLIENT APPLICATION Any OpenTelemetry tracing context present on HotRot client applications will be automatically propagated by the new Hot Rod v4 client to the server tracing context. For instance, for a client that defines some tracing spans containing cache operations, such as the following: public class MyRestClient { public void putSomeValues(RemoteCache cache) { Span span = tracer.spanBuilder("sub-bulk-1").setSpanKind(SpanKind.CLIENT).startSpan(); // put the span into the current Context try (Scope scope = span.makeCurrent()) { cache.put(1, "A"); cache.put(2, "B"); cache.put(3, "C"); } catch (Throwable throwable) { span.setStatus(StatusCode.ERROR, "Something bad happened!"); span.recordException(throwable); throw throwable; } finally { span.end(); // Cannot set a span after this call } } } The client span sub-bulk-1 will be correlated to any related server spans, in this case the three put operations. Opening the Jaeger console, we can see that client and server spans are correctly aggregated: You can find a complete application example here: TRACING FROM A REST CLIENT APPLICATION You can achieve the same with a REST client by putting manually in the HTTP headers the requests to provide information about the current tracing context using a standard OpenTelemetry instance of W3CTraceContextPropagator. public class MyRestClient { public void putSomeValues(RestCacheClient cache) { Span span = tracer.spanBuilder("sub-bulk-1").setSpanKind(SpanKind.CLIENT).startSpan(); // put the span into the current Context try (Scope scope = span.makeCurrent()) { putSomeEntries(cache); } catch (Throwable throwable) { span.setStatus(StatusCode.ERROR, "Something bad happened!"); span.recordException(throwable); throw throwable; } finally { span.end(); // Cannot set a span after this call } } private void putSomeEntries(RestCacheClient cache) { Map&lt;String, String&gt; contextMap = getContextMap(); CompletableFuture[] futures = new CompletableFuture[3]; futures[0] = cache.put("1", MediaType.TEXT_PLAIN.toString(), RestEntity.create(MediaType.TEXT_PLAIN, "A"), contextMap).toCompletableFuture(); futures[1] = cache.put("2", MediaType.TEXT_PLAIN.toString(), RestEntity.create(MediaType.TEXT_PLAIN, "B"), contextMap).toCompletableFuture(); futures[2] = cache.put("3", MediaType.TEXT_PLAIN.toString(), RestEntity.create(MediaType.TEXT_PLAIN, "C"), contextMap).toCompletableFuture(); CompletableFuture.allOf(futures).join(); } public static Map&lt;String, String&gt; getContextMap() { HashMap&lt;String, String&gt; result = new HashMap&lt;&gt;(); // Inject the request with the *current* Context, which contains our current Span. W3CTraceContextPropagator.getInstance().inject(Context.current(), result, (carrier, key, value) -&gt; carrier.put(key, value)); return result; } } Opening the Jaeger console, you can see that client and server spans are correctly aggregated: You can find a complete application example here:</content><dc:creator>Fabio Massimo Ercoli</dc:creator></entry><entry><title type="html">Kogito Serverless Workflow GitHub Extension released!</title><link rel="alternate" href="https://blog.kie.org/2022/07/new-github-extension-released.html" /><author><name>Ajay Jaganathan</name></author><id>https://blog.kie.org/2022/07/new-github-extension-released.html</id><updated>2022-07-18T12:06:21Z</updated><content type="html">We are very happy to announce the release of our Kogito Serverless Workflow Editor for GitHub extension in the . HOW TO INSTALL THE EXTENSION? A prerequisite is to have installed. You can install the extension in two ways: 1. Go to the and click on the Add to Chrome. 2. Go to the page, download and extract the chrome_extension_serverless_workflow_editor_VERSION.zip. Then in Google Chrome go to Customize and control → Settings → Extensions → Load unpacked and open the dist folder. WHAT DOES THE EXTENSION DO? You can use the Serverless Workflow editor to edit the  files in the code editor and view the Serverless Workflow diagram in the diagram visualizer. WHAT ARE ITS FEATURES? * Dynamically updates the diagram as you edit the code. * Saves the workflow diagram as .svg file in the workspace. * Auto-completes the code based on context. * Validates the code to provide an error free experience. Here is a quick video of the extension at work: Working with the Kogito Serverless Workflow Editor for GitHub extension For more information on the extension, take a look at our awesome ! Please do let us know your feedback so that we can continue improving the extension. Thanks to all the awesome engineers who helped us to achieve this!! Stay tuned to get more updates on the extension! The post appeared first on .</content><dc:creator>Ajay Jaganathan</dc:creator></entry></feed>
